{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing all the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # df processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input df files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: graphviz in c:\\users\\stanl\\anaconda3\\lib\\site-packages (0.20.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pydot in c:\\users\\stanl\\anaconda3\\lib\\site-packages (1.4.2)\n",
      "Requirement already satisfied: pyparsing>=2.1.4 in c:\\users\\stanl\\anaconda3\\lib\\site-packages (from pydot) (3.0.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install pydot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wget in c:\\users\\stanl\\anaconda3\\lib\\site-packages (3.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install wget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved under helper_prabowo_ml (1).py\n"
     ]
    }
   ],
   "source": [
    "!python -m wget https://raw.githubusercontent.com/yogawicaksana/helper_prabowo/main/helper_prabowo_ml.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.25.1-py3-none-any.whl (5.8 MB)\n",
      "Collecting huggingface-hub<1.0,>=0.10.0\n",
      "  Downloading huggingface_hub-0.11.1-py3-none-any.whl (182 kB)\n",
      "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
      "  Downloading tokenizers-0.13.2-cp39-cp39-win_amd64.whl (3.3 MB)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\stanl\\anaconda3\\lib\\site-packages (from transformers) (1.21.5)\n",
      "Requirement already satisfied: requests in c:\\users\\stanl\\anaconda3\\lib\\site-packages (from transformers) (2.27.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\stanl\\anaconda3\\lib\\site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\stanl\\anaconda3\\lib\\site-packages (from transformers) (3.6.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\stanl\\anaconda3\\lib\\site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\stanl\\anaconda3\\lib\\site-packages (from transformers) (4.64.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\stanl\\anaconda3\\lib\\site-packages (from transformers) (2022.3.15)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\stanl\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.1.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\stanl\\anaconda3\\lib\\site-packages (from packaging>=20.0->transformers) (3.0.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\stanl\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.4)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\stanl\\anaconda3\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\stanl\\anaconda3\\lib\\site-packages (from requests->transformers) (1.26.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\stanl\\anaconda3\\lib\\site-packages (from requests->transformers) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\stanl\\anaconda3\\lib\\site-packages (from requests->transformers) (2021.10.8)\n",
      "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
      "Successfully installed huggingface-hub-0.11.1 tokenizers-0.13.2 transformers-4.25.1\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.11.0-cp39-cp39-win_amd64.whl (1.9 kB)\n",
      "Collecting tensorflow-intel==2.11.0\n",
      "  Downloading tensorflow_intel-2.11.0-cp39-cp39-win_amd64.whl (266.3 MB)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\stanl\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\stanl\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (1.12.1)\n",
      "Collecting tensorflow-estimator<2.12,>=2.11.0\n",
      "  Downloading tensorflow_estimator-2.11.0-py2.py3-none-any.whl (439 kB)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\stanl\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (1.42.0)\n",
      "Collecting gast<=0.4.0,>=0.2.1\n",
      "  Downloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\users\\stanl\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (3.6.0)\n",
      "Collecting astunparse>=1.6.0\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting opt-einsum>=2.3.2\n",
      "  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "Collecting tensorboard<2.12,>=2.11\n",
      "  Downloading tensorboard-2.11.0-py3-none-any.whl (6.0 MB)\n",
      "Collecting keras<2.12,>=2.11.0\n",
      "  Downloading keras-2.11.0-py2.py3-none-any.whl (1.7 MB)\n",
      "Collecting termcolor>=1.1.0\n",
      "  Downloading termcolor-2.1.1-py3-none-any.whl (6.2 kB)\n",
      "Requirement already satisfied: numpy>=1.20 in c:\\users\\stanl\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (1.21.5)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\stanl\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (4.1.1)\n",
      "Collecting libclang>=13.0.0\n",
      "  Downloading libclang-14.0.6-py2.py3-none-win_amd64.whl (14.2 MB)\n",
      "Collecting flatbuffers>=2.0\n",
      "  Downloading flatbuffers-22.12.6-py2.py3-none-any.whl (26 kB)\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.28.0-cp39-cp39-win_amd64.whl (1.5 MB)\n",
      "Collecting google-pasta>=0.1.1\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Requirement already satisfied: protobuf<3.20,>=3.9.2 in c:\\users\\stanl\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (3.19.1)\n",
      "Collecting absl-py>=1.0.0\n",
      "  Downloading absl_py-1.3.0-py3-none-any.whl (124 kB)\n",
      "Requirement already satisfied: setuptools in c:\\users\\stanl\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (61.2.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\stanl\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (21.3)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\stanl\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.11.0->tensorflow) (0.37.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\stanl\\anaconda3\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (2.27.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\stanl\\anaconda3\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (2.0.3)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\stanl\\anaconda3\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (3.3.4)\n",
      "Collecting tensorboard-data-server<0.7.0,>=0.6.0\n",
      "  Downloading tensorboard_data_server-0.6.1-py3-none-any.whl (2.4 kB)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\stanl\\anaconda3\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (1.33.0)\n",
      "Collecting tensorboard-plugin-wit>=1.6.0\n",
      "  Downloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n",
      "Collecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in c:\\users\\stanl\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (4.2.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\stanl\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (4.7.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\stanl\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (0.2.8)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Downloading requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\stanl\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\stanl\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\stanl\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (1.26.9)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\stanl\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\stanl\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (2021.10.8)\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Downloading oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\stanl\\anaconda3\\lib\\site-packages (from packaging->tensorflow-intel==2.11.0->tensorflow) (3.0.4)\n",
      "Installing collected packages: oauthlib, requests-oauthlib, tensorboard-plugin-wit, tensorboard-data-server, google-auth-oauthlib, absl-py, termcolor, tensorflow-io-gcs-filesystem, tensorflow-estimator, tensorboard, opt-einsum, libclang, keras, google-pasta, gast, flatbuffers, astunparse, tensorflow-intel, tensorflow\n",
      "Successfully installed absl-py-1.3.0 astunparse-1.6.3 flatbuffers-22.12.6 gast-0.4.0 google-auth-oauthlib-0.4.6 google-pasta-0.2.0 keras-2.11.0 libclang-14.0.6 oauthlib-3.2.2 opt-einsum-3.3.0 requests-oauthlib-1.3.1 tensorboard-2.11.0 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 tensorflow-2.11.0 tensorflow-estimator-2.11.0 tensorflow-intel-2.11.0 tensorflow-io-gcs-filesystem-0.28.0 termcolor-2.1.1\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from helper_prabowo_ml import clean_html, remove_links, non_ascii, lower, email_address, removeStopWords, punct, remove_, remove_special_characters, remove_digits\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings, re\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoTokenizer, TFBertModel\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Input, GlobalMaxPool1D, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.initializers import TruncatedNormal\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "from tensorflow.keras.metrics import CategoricalAccuracy\n",
    "from sklearn.metrics import classification_report\n",
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "from tensorflow.keras.optimizers.legacy import Adam as legacyAdam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the train, validation and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"data/train.txt\",names=['Input','Sentiment'],sep=';',encoding='utf-8')\n",
    "val = pd.read_csv(\"data/val.txt\",names=['Input','Sentiment'],sep=';',encoding='utf-8')\n",
    "test = pd.read_csv(\"data/test.txt\",names=['Input','Sentiment'],sep=';',encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining the train, validation and test datasets into a single dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>Input</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>210</td>\n",
       "      <td>i feel pretty rotten</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3801</td>\n",
       "      <td>i feel more irritated than peaceful</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9442</td>\n",
       "      <td>i know that feeling myself the strange sense o...</td>\n",
       "      <td>fear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14632</td>\n",
       "      <td>i just ran by feel and i m glad i didn t look ...</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4008</td>\n",
       "      <td>i have the feeling i am going to be tortured t...</td>\n",
       "      <td>fear</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index                                              Input Sentiment\n",
       "0    210                               i feel pretty rotten   sadness\n",
       "1   3801                i feel more irritated than peaceful     anger\n",
       "2   9442  i know that feeling myself the strange sense o...      fear\n",
       "3  14632  i just ran by feel and i m glad i didn t look ...       joy\n",
       "4   4008  i have the feeling i am going to be tortured t...      fear"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.concat([train,val,test],axis=0)\n",
    "df = df.sample(frac=0.1)\n",
    "df = df.reset_index()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.drop('index',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 2)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocess_data(data,col):\n",
    "    data[col] = data[col].apply(func=clean_html)\n",
    "    data[col] = data[col].apply(func=remove_digits)\n",
    "    data[col] = data[col].apply(func=remove_)\n",
    "    data[col] = data[col].apply(func=removeStopWords)\n",
    "    data[col] = data[col].apply(func=remove_links)\n",
    "    data[col] = data[col].apply(func=remove_special_characters)\n",
    "    data[col] = data[col].apply(func=non_ascii)\n",
    "    data[col] = data[col].apply(func=email_address)\n",
    "    data[col] = data[col].apply(func=punct)\n",
    "    data[col] = data[col].apply(func=lower)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Input</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>feel pretty rotte</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>feel irritated peacefu</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ow feeling strange sense serendipity minds col...</td>\n",
       "      <td>fear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ran feel glad look probably would freaked happ...</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>feeling going tortured tonight</td>\n",
       "      <td>fear</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Input Sentiment\n",
       "0                                  feel pretty rotte   sadness\n",
       "1                             feel irritated peacefu     anger\n",
       "2  ow feeling strange sense serendipity minds col...      fear\n",
       "3  ran feel glad look probably would freaked happ...       joy\n",
       "4                     feeling going tortured tonight      fear"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed_df = preprocess_data(df,'Input')\n",
    "preprocessed_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "preprocessed_df['num_words'] = preprocessed_df.Input.apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Input</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>num_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>feel pretty rotte</td>\n",
       "      <td>sadness</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>feel irritated peacefu</td>\n",
       "      <td>anger</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ow feeling strange sense serendipity minds col...</td>\n",
       "      <td>fear</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ran feel glad look probably would freaked happ...</td>\n",
       "      <td>joy</td>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>feeling going tortured tonight</td>\n",
       "      <td>fear</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Input Sentiment  num_words\n",
       "0                                  feel pretty rotte   sadness         17\n",
       "1                             feel irritated peacefu     anger         22\n",
       "2  ow feeling strange sense serendipity minds col...      fear         56\n",
       "3  ran feel glad look probably would freaked happ...       joy         68\n",
       "4                     feeling going tortured tonight      fear         30"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "encoded_labels = {'anger': 0, 'fear': 1, 'joy': 2, 'love': 3, 'sadness': 4, 'surprise': 5} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_data, test_data = train_test_split(preprocessed_df,test_size=0.3,random_state=101,shuffle=True,stratify=preprocessed_df.Sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Tokenizer class and pretrained BERT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "bert_model = TFBertModel.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<seaborn.axisgrid.FacetGrid at 0x20c7cf861f0>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.displot(preprocessed_df.num_words,height=8,aspect=1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "max_len = 40"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train = tokenizer(text=train_data.Input.tolist(),\n",
    "                   add_special_tokens=True,\n",
    "                   return_tensors='tf',\n",
    "                   max_length=max_len,\n",
    "                   padding=True,\n",
    "                   truncation=True,\n",
    "                   return_token_type_ids=False,\n",
    "                   return_attention_mask=True,\n",
    "                   verbose=True\n",
    "                   )\n",
    "\n",
    "X_test = tokenizer(text=test_data.Input.tolist(),\n",
    "                   add_special_tokens=True,\n",
    "                   return_tensors='tf',\n",
    "                   max_length=max_len,\n",
    "                   padding=True,\n",
    "                   truncation=True,\n",
    "                   return_token_type_ids=False,\n",
    "                   return_attention_mask=True,\n",
    "                   verbose=True\n",
    "                  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "input_ids = Input(shape=(max_len,),name='input_ids',dtype=tf.int32)\n",
    "attention_mask = Input(shape=(max_len,),name='attention_mask',dtype=tf.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "embeddings = bert_model(input_ids,attention_mask=attention_mask)[0] # 0: final hidden state, 1: pooling output\n",
    "output = GlobalMaxPool1D()(embeddings)\n",
    "output = Dense(units=128,activation='relu')(output)\n",
    "output = Dropout(0.1)(output)\n",
    "output = Dense(units=64,activation='relu')(output)\n",
    "output = Dense(units=32,activation='relu')(output)\n",
    "y = Dense(units=6,activation='softmax')(output)\n",
    "\n",
    "model = Model(inputs=[input_ids,attention_mask],outputs=y)\n",
    "model.layers[2].trainable = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compiling the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.compile(loss=CategoricalCrossentropy(from_logits=True),\n",
    "              optimizer=legacyAdam(learning_rate=5e-5,epsilon=1e-8,decay=0.01,clipnorm=1.0),\n",
    "              metrics=CategoricalAccuracy('balanced_accuracy'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding the emotion labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_data['Label'] = train_data.Sentiment.map(encoded_labels)\n",
    "test_data['Label'] = test_data.Sentiment.map(encoded_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Input</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>num_words</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>446</th>\n",
       "      <td>ve gotten used extent im actually feeling weir...</td>\n",
       "      <td>surprise</td>\n",
       "      <td>55</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1838</th>\n",
       "      <td>feel intimidated girls acne getting rid pimples</td>\n",
       "      <td>fear</td>\n",
       "      <td>47</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1423</th>\n",
       "      <td>want snap simply feel simply live laugh enjoy ...</td>\n",
       "      <td>sadness</td>\n",
       "      <td>65</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1578</th>\n",
       "      <td>push back repressed expressions child split ma...</td>\n",
       "      <td>anger</td>\n",
       "      <td>152</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1326</th>\n",
       "      <td>ow later read ill feel regretful ive posted th...</td>\n",
       "      <td>sadness</td>\n",
       "      <td>62</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Input Sentiment  num_words  \\\n",
       "446   ve gotten used extent im actually feeling weir...  surprise         55   \n",
       "1838    feel intimidated girls acne getting rid pimples      fear         47   \n",
       "1423  want snap simply feel simply live laugh enjoy ...   sadness         65   \n",
       "1578  push back repressed expressions child split ma...     anger        152   \n",
       "1326  ow later read ill feel regretful ive posted th...   sadness         62   \n",
       "\n",
       "      Label  \n",
       "446       5  \n",
       "1838      1  \n",
       "1423      4  \n",
       "1578      0  \n",
       "1326      4  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Input</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>num_words</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1310</th>\n",
       "      <td>still remember teenager giddy feeling amazemen...</td>\n",
       "      <td>love</td>\n",
       "      <td>109</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1635</th>\n",
       "      <td>m trying positive feel positive</td>\n",
       "      <td>joy</td>\n",
       "      <td>31</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1078</th>\n",
       "      <td>feel people shamed</td>\n",
       "      <td>sadness</td>\n",
       "      <td>18</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>337</th>\n",
       "      <td>remember going shandur highest polo ground wor...</td>\n",
       "      <td>sadness</td>\n",
       "      <td>154</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1654</th>\n",
       "      <td>ve struggled feeling inadequate subpar various...</td>\n",
       "      <td>sadness</td>\n",
       "      <td>69</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Input Sentiment  num_words  \\\n",
       "1310  still remember teenager giddy feeling amazemen...      love        109   \n",
       "1635                    m trying positive feel positive       joy         31   \n",
       "1078                                 feel people shamed   sadness         18   \n",
       "337   remember going shandur highest polo ground wor...   sadness        154   \n",
       "1654  ve struggled feeling inadequate subpar various...   sadness         69   \n",
       "\n",
       "      Label  \n",
       "1310      3  \n",
       "1635      2  \n",
       "1078      4  \n",
       "337       4  \n",
       "1654      4  "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating the model summary and plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_ids (InputLayer)         [(None, 40)]         0           []                               \n",
      "                                                                                                  \n",
      " attention_mask (InputLayer)    [(None, 40)]         0           []                               \n",
      "                                                                                                  \n",
      " tf_bert_model_1 (TFBertModel)  TFBaseModelOutputWi  109482240   ['input_ids[0][0]',              \n",
      "                                thPoolingAndCrossAt               'attention_mask[0][0]']         \n",
      "                                tentions(last_hidde                                               \n",
      "                                n_state=(None, 40,                                                \n",
      "                                768),                                                             \n",
      "                                 pooler_output=(Non                                               \n",
      "                                e, 768),                                                          \n",
      "                                 past_key_values=No                                               \n",
      "                                ne, hidden_states=N                                               \n",
      "                                one, attentions=Non                                               \n",
      "                                e, cross_attentions                                               \n",
      "                                =None)                                                            \n",
      "                                                                                                  \n",
      " global_max_pooling1d_1 (Global  (None, 768)         0           ['tf_bert_model_1[0][0]']        \n",
      " MaxPooling1D)                                                                                    \n",
      "                                                                                                  \n",
      " dense_4 (Dense)                (None, 128)          98432       ['global_max_pooling1d_1[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_75 (Dropout)           (None, 128)          0           ['dense_4[0][0]']                \n",
      "                                                                                                  \n",
      " dense_5 (Dense)                (None, 64)           8256        ['dropout_75[0][0]']             \n",
      "                                                                                                  \n",
      " dense_6 (Dense)                (None, 32)           2080        ['dense_5[0][0]']                \n",
      "                                                                                                  \n",
      " dense_7 (Dense)                (None, 6)            198         ['dense_6[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 109,591,206\n",
      "Trainable params: 109,591,206\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n"
     ]
    }
   ],
   "source": [
    "plot_model(model,'model.png',show_shapes=True,dpi=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and fine-tuning the pretrained BERT model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_1/bert/pooler/dense/kernel:0', 'tf_bert_model_1/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_1/bert/pooler/dense/kernel:0', 'tf_bert_model_1/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "44/44 [==============================] - ETA: 0s - loss: 1.5278 - balanced_accuracy: 0.4271"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"C:\\Users\\stanl\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1820, in test_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\stanl\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1804, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\stanl\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1792, in run_step  **\n        outputs = model.test_step(data)\n    File \"C:\\Users\\stanl\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1756, in test_step\n        y_pred = self(x, training=False)\n    File \"C:\\Users\\stanl\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"C:\\Users\\stanl\\anaconda3\\lib\\site-packages\\keras\\engine\\input_spec.py\", line 295, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer \"model_1\" is incompatible with the layer: expected shape=(None, 40), found shape=(None, 35)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [55]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mattention_mask\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mattention_mask\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m             \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mto_categorical\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLabel\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m             \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m             \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m             \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mattention_mask\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mattention_mask\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43mto_categorical\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLabel\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m             \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_filehlrijd3d.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__test_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"C:\\Users\\stanl\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1820, in test_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\stanl\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1804, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\stanl\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1792, in run_step  **\n        outputs = model.test_step(data)\n    File \"C:\\Users\\stanl\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1756, in test_step\n        y_pred = self(x, training=False)\n    File \"C:\\Users\\stanl\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"C:\\Users\\stanl\\anaconda3\\lib\\site-packages\\keras\\engine\\input_spec.py\", line 295, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer \"model_1\" is incompatible with the layer: expected shape=(None, 40), found shape=(None, 35)\n"
     ]
    }
   ],
   "source": [
    "r = model.fit(x={'input_ids': X_train['input_ids'], 'attention_mask': X_train['attention_mask']},\n",
    "             y=to_categorical(train_data.Label),\n",
    "             epochs=10,\n",
    "             batch_size=32,\n",
    "             validation_data=({'input_ids': X_test['input_ids'], 'attention_mask': X_test['attention_mask']},to_categorical(test_data.Label))\n",
    "             )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-13T18:05:01.090755Z",
     "iopub.status.busy": "2022-12-13T18:05:01.090348Z",
     "iopub.status.idle": "2022-12-13T18:05:01.341971Z",
     "shell.execute_reply": "2022-12-13T18:05:01.340961Z",
     "shell.execute_reply.started": "2022-12-13T18:05:01.090718Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "plt.plot(r.history['loss'],'r',label='train loss')\n",
    "plt.plot(r.history['val_loss'],'b',label='test loss')\n",
    "plt.xlabel('No. of Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss Graph')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-13T18:05:01.347917Z",
     "iopub.status.busy": "2022-12-13T18:05:01.347043Z",
     "iopub.status.idle": "2022-12-13T18:05:01.587962Z",
     "shell.execute_reply": "2022-12-13T18:05:01.587052Z",
     "shell.execute_reply.started": "2022-12-13T18:05:01.34788Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "plt.plot(r.history['balanced_accuracy'],'r',label='train accuracy')\n",
    "plt.plot(r.history['val_balanced_accuracy'],'b',label='test accuracy')\n",
    "plt.xlabel('No. of Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Accuracy Graph')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-13T18:05:18.984902Z",
     "iopub.status.busy": "2022-12-13T18:05:18.984526Z",
     "iopub.status.idle": "2022-12-13T18:05:22.792132Z",
     "shell.execute_reply": "2022-12-13T18:05:22.790967Z",
     "shell.execute_reply.started": "2022-12-13T18:05:18.984872Z"
    }
   },
   "outputs": [],
   "source": [
    "model.save(\"emotion_detector.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the model on the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-13T18:05:22.795044Z",
     "iopub.status.busy": "2022-12-13T18:05:22.794443Z",
     "iopub.status.idle": "2022-12-13T18:05:28.759338Z",
     "shell.execute_reply": "2022-12-13T18:05:28.758346Z",
     "shell.execute_reply.started": "2022-12-13T18:05:22.795003Z"
    }
   },
   "outputs": [],
   "source": [
    "loss, acc = model.evaluate({'input_ids': X_test['input_ids'], 'attention_mask': X_test['attention_mask']},to_categorical(test_data.Label))\n",
    "print(\"Test Categorical Cross-Entropy Loss:\",loss)\n",
    "print(\"Test Categorical Accuracy:\",acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-13T18:05:28.761706Z",
     "iopub.status.busy": "2022-12-13T18:05:28.76101Z",
     "iopub.status.idle": "2022-12-13T18:05:31.759064Z",
     "shell.execute_reply": "2022-12-13T18:05:31.757861Z",
     "shell.execute_reply.started": "2022-12-13T18:05:28.761668Z"
    }
   },
   "outputs": [],
   "source": [
    "test_predictions = model.predict({'input_ids': X_test['input_ids'], 'attention_mask': X_test['attention_mask']})\n",
    "test_predictions = np.argmax(test_predictions,axis=1)\n",
    "print(classification_report(test_data.Label,test_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is quite promising as we've obtained an excellent f1-score of almost 80% for all the 6 emotion classes."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
